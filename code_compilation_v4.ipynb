{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1tPsGURUyw6y6HWDalIc_A63KRZtO55lC","timestamp":1717666146090}],"gpuType":"T4","authorship_tag":"ABX9TyODIp7y59voexNBN97Tgxfa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Compiling code\n","\n","1.   Object detection in Video\n","2.   Eye blink detection\n","3.   Face recognition\n","\n"],"metadata":{"id":"-VDLaQ_MWWO8"}},{"cell_type":"markdown","source":["Mounting google drive"],"metadata":{"id":"HNR1jRB3cQhD"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"yK3D_B1EWVPX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717744232763,"user_tz":-330,"elapsed":21715,"user":{"displayName":"atul kashyap","userId":"02181211827561551789"}},"outputId":"31ce707b-e465-4b28-acdc-85fba35b7b63"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","path = '/content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled'\n","os.chdir(path)"]},{"cell_type":"markdown","source":["Importing the required libraries"],"metadata":{"id":"anWGrmEfcik2"}},{"cell_type":"code","source":["!pip install ultralytics\n","!pip install deepface\n","# !pip install \"/content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/Dependencies/dlib-19.24.1-cp311-cp311-win_amd64.whl\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"F2dXeviacwOk","executionInfo":{"status":"ok","timestamp":1717744319003,"user_tz":-330,"elapsed":86242,"user":{"displayName":"atul kashyap","userId":"02181211827561551789"}},"outputId":"9146be1f-3876-44db-8e33-ff8207d2532b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ultralytics\n","  Downloading ultralytics-8.2.28-py3-none-any.whl (779 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.6/779.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.4)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.3.0+cu121)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.18.0+cu121)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.4)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.3)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n","Collecting ultralytics-thop>=0.2.5 (from ultralytics)\n","  Downloading ultralytics_thop-0.2.7-py3-none-any.whl (25 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.53.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.6.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.3.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics)\n","  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 ultralytics-8.2.28 ultralytics-thop-0.2.7\n","Collecting deepface\n","  Downloading deepface-0.0.91-py3-none-any.whl (97 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests>=2.27.1 in /usr/local/lib/python3.10/dist-packages (from deepface) (2.31.0)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (1.25.2)\n","Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.10/dist-packages (from deepface) (2.0.3)\n","Requirement already satisfied: gdown>=3.10.1 in /usr/local/lib/python3.10/dist-packages (from deepface) (5.1.0)\n","Requirement already satisfied: tqdm>=4.30.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (4.66.4)\n","Requirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (9.4.0)\n","Requirement already satisfied: opencv-python>=4.5.5.64 in /usr/local/lib/python3.10/dist-packages (from deepface) (4.8.0.76)\n","Requirement already satisfied: tensorflow>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (2.15.0)\n","Requirement already satisfied: keras>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (2.15.0)\n","Requirement already satisfied: Flask>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from deepface) (2.2.5)\n","Collecting mtcnn>=0.1.0 (from deepface)\n","  Downloading mtcnn-0.1.1-py3-none-any.whl (2.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting retina-face>=0.0.1 (from deepface)\n","  Downloading retina_face-0.0.17-py3-none-any.whl (25 kB)\n","Collecting fire>=0.4.0 (from deepface)\n","  Downloading fire-0.6.0.tar.gz (88 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting gunicorn>=20.1.0 (from deepface)\n","  Downloading gunicorn-22.0.0-py3-none-any.whl (84 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire>=0.4.0->deepface) (1.16.0)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire>=0.4.0->deepface) (2.4.0)\n","Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.1.2->deepface) (3.0.3)\n","Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.1.2->deepface) (3.1.4)\n","Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.1.2->deepface) (2.2.0)\n","Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.1.2->deepface) (8.1.7)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=3.10.1->deepface) (4.12.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=3.10.1->deepface) (3.14.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gunicorn>=20.1.0->deepface) (24.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->deepface) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->deepface) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->deepface) (2024.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->deepface) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->deepface) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->deepface) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->deepface) (2024.6.2)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (3.9.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (18.1.1)\n","Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (0.2.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (3.3.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (67.7.2)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (4.12.1)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (0.37.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (1.64.1)\n","Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (2.15.2)\n","Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (2.15.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=1.9.0->deepface) (0.43.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=1.1.2->deepface) (2.1.5)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=1.9.0->deepface) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=1.9.0->deepface) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=1.9.0->deepface) (3.6)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=1.9.0->deepface) (0.7.2)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=3.10.1->deepface) (2.5)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->deepface) (1.7.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=1.9.0->deepface) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=1.9.0->deepface) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=1.9.0->deepface) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=1.9.0->deepface) (1.3.1)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=1.9.0->deepface) (0.6.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=1.9.0->deepface) (3.2.2)\n","Building wheels for collected packages: fire\n","  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117029 sha256=a08077e1f53dfa894fe382ed71e18b52497acb33b65e15559edef85cb55761b4\n","  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n","Successfully built fire\n","Installing collected packages: gunicorn, fire, mtcnn, retina-face, deepface\n","Successfully installed deepface-0.0.91 fire-0.6.0 gunicorn-22.0.0 mtcnn-0.1.1 retina-face-0.0.17\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","\n","import cv2\n","from google.colab.patches import cv2_imshow\n","from ultralytics import YOLO\n","\n","import dlib\n","from imutils import face_utils\n","from scipy.spatial import distance as dist\n","\n","from deepface import DeepFace\n","\n","import time\n","\n","# Start timer\n","tic = time.time()\n","\n","# Load the pre-trained model for YOLOv8\n","model = YOLO('yolov8m.pt')\n","\n","# Path to the video\n","video_path = '/content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4'\n","\n","# Run the video through the model\n","results = model(source=video_path, show=True, conf=0.4, save=True)\n","\n","# Storing the required information in the dataframe\n","\n","import pandas as pd\n","frames_info = pd.DataFrame(columns = ['frame_id', 'conf_score', 'object_id', 'object_name', 'x1', 'x2', 'y1', 'y2'])\n","\n","count = 1\n","\n","for result in results:\n","  for detect in result.boxes.data:\n","    detect = detect.cpu().numpy()\n","    t = pd.DataFrame({\n","        'frame_id':[count],\n","        'conf_score':[detect[4]],\n","        'object_id':[int(detect[5])],\n","        'object_name':[model.names[int(detect[5])]],\n","        'x1':[detect[0]],\n","        'x2':[detect[2]],\n","        'y1':[detect[1]],\n","        'y2':[detect[3]]\n","    })\n","\n","    frames_info = pd.concat([frames_info, t], ignore_index = True)\n","  count +=1\n","\n","tc = time.time()\n","print(tc-tic)\n","\n","# Initialize an empty set to store unique object names\n","detected_objects = set()\n","\n","# Process the results\n","for result in results:\n","    for detection in result.boxes.data:\n","        class_id = int(detection[5])\n","        class_name = model.names[class_id]\n","        detected_objects.add(class_name)\n","\n","# Convert the set to a list to get the final list of detected objects\n","objects_list = list(detected_objects)\n","print(objects_list)\n","print('\\n')\n","\n","# Check for unauthorized objects\n","objects_of_interest = [\"cell phone\"]  # \"laptop\"\n","if any(obj in objects_list for obj in objects_of_interest):\n","    print(\"UNAUTHORISED ACCESS: Phone/Laptop is detected.\")\n","else:\n","    print('Moving to eye-blink detection.\\n')\n","\n","    def eye_aspect_ratio(eye):\n","        A = dist.euclidean(eye[1], eye[5])\n","        B = dist.euclidean(eye[2], eye[4])\n","        C = dist.euclidean(eye[0], eye[3])\n","        ear = (A + B) / (2.0 * C)\n","        return ear\n","\n","    EYE_AR_THRESH = 0.21\n","    EYE_AR_CONSEC_FRAMES = 5\n","\n","    # Initialize counters\n","    COUNTER = 0\n","    TOTAL = 0\n","\n","    # Initialize dlib's face detector and create the facial landmark predictor\n","    detector = dlib.get_frontal_face_detector()\n","    predictor = dlib.shape_predictor('/content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/Dependencies/shape_predictor_68_face_landmarks.dat')\n","\n","    (lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n","    (rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]\n","\n","    # Open the video file again for eye blink detection\n","    cap = cv2.VideoCapture(video_path)\n","\n","    # Get the width and height of the frames in the video\n","    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","\n","    # Define the codec and create VideoWriter object\n","    output_path = '/content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/outputs/output - Ashish.avi'\n","    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'XVID'), 10, (frame_width, frame_height))\n","\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","\n","        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","        rects = detector(gray, 0)\n","\n","        for rect in rects:\n","            shape = predictor(gray, rect)\n","            shape = face_utils.shape_to_np(shape)\n","\n","            leftEye = shape[lStart:lEnd]\n","            rightEye = shape[rStart:rEnd]\n","            leftEAR = eye_aspect_ratio(leftEye)\n","            rightEAR = eye_aspect_ratio(rightEye)\n","\n","            ear = (leftEAR + rightEAR) / 2.0\n","\n","            leftEyeHull = cv2.convexHull(leftEye)\n","            rightEyeHull = cv2.convexHull(rightEye)\n","            cv2.drawContours(frame, [leftEyeHull], -1, (0, 255, 0), 1)\n","            cv2.drawContours(frame, [rightEyeHull], -1, (0, 255, 0), 1)\n","\n","            if ear < EYE_AR_THRESH:\n","                COUNTER += 1\n","            else:\n","                if COUNTER >= EYE_AR_CONSEC_FRAMES:\n","                    TOTAL += 1\n","                COUNTER = 0\n","\n","            cv2.putText(frame, \"Blinks: {}\".format(TOTAL), (10, 30),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 0, 255), 2)\n","\n","        # Write the frame to the output video file\n","        out.write(frame)\n","\n","        if cv2.waitKey(1) & 0xFF == ord('q'):\n","            break\n","\n","    cap.release()\n","    out.release()\n","    cv2.destroyAllWindows()\n","\n","    # Print the final number of blinks\n","    print(f\"Total number of blinks detected: {TOTAL}\")\n","\n","    if not TOTAL:\n","        print('UNAUTHORISED ACCESS: Spoofing detected')\n","    else:\n","        print('Moving to face detection.')\n","\n","        # Initialize variables to store the highest scoring person object and its corresponding frame\n","        highest_score = 0\n","        person_bbox_with_highest_score = None\n","        frame_with_highest_score = None\n","\n","        # Open the video file again for face detection\n","        cap = cv2.VideoCapture(video_path)\n","\n","        # Iterate over the detected frames\n","        for frame_idx, frame_results in enumerate(results):\n","            for detection in frame_results.boxes.data:\n","                x1, y1, x2, y2, conf, cls = detection\n","\n","                # Check if the detected object is a person (assuming class '0' corresponds to 'person')\n","                if int(cls) == 0 and conf > highest_score:\n","                    highest_score = conf\n","                    person_bbox_with_highest_score = (int(x1), int(y1), int(x2), int(y2))\n","\n","                    # Set the video frame to the current frame index\n","                    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n","                    ret, frame_with_highest_score = cap.read()\n","\n","        cap.release()\n","\n","        # If a person object with the highest score is found, crop the corresponding region from the frame\n","        if person_bbox_with_highest_score is not None and frame_with_highest_score is not None:\n","            x1, y1, x2, y2 = person_bbox_with_highest_score\n","            cropped_person = frame_with_highest_score[y1:y2, x1:x2]\n","            # Display or save the cropped person image\n","            print(f'Frame no. {frame_idx}: {highest_score}')\n","            # cv2_imshow(cropped_person)\n","            cv2.waitKey(0)\n","            cv2.destroyAllWindows()\n","\n","\n","            # --------- FACE RECOGNITION ---------\n","\n","            # Path to the database directory\n","            # db_path = \"/content/drive/MyDrive/Colab Notebooks/ND Face Recognition/Faces/train/AshishGhule\"\n","            db_path = \"/content/drive/MyDrive/Colab Notebooks/ND Face Recognition/Faces/train/AkshayFadnavis\"\n","            suffixes = (\"png\", \"jpg\", \"jpeg\")\n","\n","            # Initialize variables\n","            is_verified = False\n","            threshold_value = None\n","            matched_image_path = None\n","            my_img_dist = None\n","\n","            for i in os.listdir(db_path):\n","              if i.lower().endswith(suffixes):\n","                my_img_path = os.path.join(db_path, i)\n","                try:\n","                  vf = DeepFace.verify(cropped_person, my_img_path, model_name = \"Facenet512\", enforce_detection = False)\n","                  if vf['distance'] < vf['threshold']:\n","                    is_verified = True\n","                    thres = vf['threshold']\n","                    matched_image_path = my_img_path\n","                    my_img_dist = vf['distance']\n","                    break\n","                except Exception as e:\n","                  print(f\"Error processing {my_img_path}: {e}\")\n","                  continue\n","\n","            if is_verified:\n","              print(\"ACCESS GRANTED: Face verified!\")\n","            else:\n","              print(\"ACCESS DENIED: Face not verified!\")\n","\n","toc = time.time()\n","print(f'Total time required for code running: {round((toc-tic)/60, 2)} Minutes.')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jJmDUop15-Vs","executionInfo":{"status":"ok","timestamp":1717744526971,"user_tz":-330,"elapsed":207972,"user":{"displayName":"atul kashyap","userId":"02181211827561551789"}},"outputId":"3416fdff-ee56-4075-9673-81275edb2aaa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["24-06-07 07:12:09 - Directory /root/.deepface created\n","24-06-07 07:12:09 - Directory /root/.deepface/weights created\n","WARNING ⚠️ Environment does not support cv2.imshow() or PIL Image.show()\n","\n","\n","\n","WARNING ⚠️ inference results will accumulate in RAM unless `stream=True` is passed, causing potential out-of-memory\n","errors for large sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.\n","\n","Example:\n","    results = model(source=..., stream=True)  # generator of Results objects\n","    for r in results:\n","        boxes = r.boxes  # Boxes object for bbox outputs\n","        masks = r.masks  # Masks object for segment masks outputs\n","        probs = r.probs  # Class probabilities for classification outputs\n","\n","video 1/1 (frame 1/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 2 keyboards, 1246.3ms\n","video 1/1 (frame 2/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 2 keyboards, 1133.0ms\n","video 1/1 (frame 3/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 laptop, 2 keyboards, 1126.3ms\n","video 1/1 (frame 4/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 laptop, 2 keyboards, 1135.1ms\n","video 1/1 (frame 5/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 1 keyboard, 1084.1ms\n","video 1/1 (frame 6/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 1 keyboard, 1392.8ms\n","video 1/1 (frame 7/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 laptop, 1 keyboard, 1078.0ms\n","video 1/1 (frame 8/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 2 keyboards, 1779.6ms\n","video 1/1 (frame 9/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 1 keyboard, 1769.3ms\n","video 1/1 (frame 10/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 2 chairs, 1 laptop, 2 keyboards, 1763.1ms\n","video 1/1 (frame 11/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 2 chairs, 1 laptop, 2 keyboards, 1099.8ms\n","video 1/1 (frame 12/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 2 keyboards, 1090.3ms\n","video 1/1 (frame 13/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 2 chairs, 1 laptop, 1 keyboard, 1066.6ms\n","video 1/1 (frame 14/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 1 keyboard, 1103.0ms\n","video 1/1 (frame 15/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 2 chairs, 1 laptop, 1 keyboard, 1054.3ms\n","video 1/1 (frame 16/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 2 chairs, 1 laptop, 2 keyboards, 1093.4ms\n","video 1/1 (frame 17/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 2 chairs, 1 laptop, 2 keyboards, 1106.0ms\n","video 1/1 (frame 18/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 2 chairs, 1 laptop, 2 keyboards, 1103.2ms\n","video 1/1 (frame 19/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 2 keyboards, 1343.2ms\n","video 1/1 (frame 20/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 2 keyboards, 1701.4ms\n","video 1/1 (frame 21/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 2 chairs, 1 laptop, 2 keyboards, 1726.5ms\n","video 1/1 (frame 22/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 2 chairs, 1 laptop, 1 keyboard, 1343.6ms\n","video 1/1 (frame 23/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 2 chairs, 1 laptop, 1 keyboard, 1059.4ms\n","video 1/1 (frame 24/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 1 keyboard, 1077.4ms\n","video 1/1 (frame 25/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 1 keyboard, 1099.3ms\n","video 1/1 (frame 26/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 2 chairs, 1 laptop, 1 keyboard, 1113.6ms\n","video 1/1 (frame 27/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 1 keyboard, 1078.8ms\n","video 1/1 (frame 28/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 laptop, 1 keyboard, 1107.2ms\n","video 1/1 (frame 29/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 2 chairs, 1 laptop, 1 keyboard, 1106.2ms\n","video 1/1 (frame 30/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 2 chairs, 1 laptop, 1 keyboard, 1032.2ms\n","video 1/1 (frame 31/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 1 keyboard, 1572.6ms\n","video 1/1 (frame 32/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 2 chairs, 1 laptop, 1 keyboard, 1734.6ms\n","video 1/1 (frame 33/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 1 keyboard, 1803.1ms\n","video 1/1 (frame 34/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 2 keyboards, 1143.4ms\n","video 1/1 (frame 35/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 2 keyboards, 1088.2ms\n","video 1/1 (frame 36/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 laptop, 2 keyboards, 1090.5ms\n","video 1/1 (frame 37/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 laptop, 2 keyboards, 1084.8ms\n","video 1/1 (frame 38/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 2 keyboards, 1079.6ms\n","video 1/1 (frame 39/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 2 keyboards, 1048.6ms\n","video 1/1 (frame 40/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 2 keyboards, 1061.3ms\n","video 1/1 (frame 41/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 laptop, 2 keyboards, 1094.3ms\n","video 1/1 (frame 42/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 laptop, 1 keyboard, 1057.2ms\n","video 1/1 (frame 43/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 2 keyboards, 1727.3ms\n","video 1/1 (frame 44/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 2 keyboards, 1741.4ms\n","video 1/1 (frame 45/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 1 keyboard, 1735.3ms\n","video 1/1 (frame 46/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 2 keyboards, 1098.0ms\n","video 1/1 (frame 47/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 1 keyboard, 1058.7ms\n","video 1/1 (frame 48/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 2 keyboards, 1677.5ms\n","video 1/1 (frame 49/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 1 keyboard, 1340.9ms\n","video 1/1 (frame 50/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 2 keyboards, 1034.0ms\n","video 1/1 (frame 51/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 2 keyboards, 1036.8ms\n","video 1/1 (frame 52/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 2 keyboards, 1096.5ms\n","video 1/1 (frame 53/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 2 keyboards, 1045.7ms\n","video 1/1 (frame 54/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 2 keyboards, 1709.7ms\n","video 1/1 (frame 55/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 2 keyboards, 1697.4ms\n","video 1/1 (frame 56/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 2 keyboards, 1707.0ms\n","video 1/1 (frame 57/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 2 keyboards, 1052.7ms\n","video 1/1 (frame 58/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 2 keyboards, 1048.0ms\n","video 1/1 (frame 59/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 2 keyboards, 1038.6ms\n","video 1/1 (frame 60/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 2 keyboards, 1047.9ms\n","video 1/1 (frame 61/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 2 chairs, 1 laptop, 2 keyboards, 1088.5ms\n","video 1/1 (frame 62/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 2 keyboards, 1071.3ms\n","video 1/1 (frame 63/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 2 keyboards, 1079.9ms\n","video 1/1 (frame 64/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 2 keyboards, 1063.0ms\n","video 1/1 (frame 65/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 2 keyboards, 1114.0ms\n","video 1/1 (frame 66/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 2 keyboards, 1714.5ms\n","video 1/1 (frame 67/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 2 keyboards, 1716.4ms\n","video 1/1 (frame 68/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 2 chairs, 1 laptop, 2 keyboards, 1522.2ms\n","video 1/1 (frame 69/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 2 keyboards, 1042.6ms\n","video 1/1 (frame 70/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 2 keyboards, 1051.3ms\n","video 1/1 (frame 71/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 2 chairs, 1 laptop, 2 keyboards, 1057.3ms\n","video 1/1 (frame 72/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 2 keyboards, 1055.4ms\n","video 1/1 (frame 73/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 2 chairs, 1 laptop, 2 keyboards, 1063.5ms\n","video 1/1 (frame 74/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 2 chairs, 1 laptop, 2 keyboards, 1013.4ms\n","video 1/1 (frame 75/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 2 keyboards, 1038.1ms\n","video 1/1 (frame 76/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 2 chairs, 1 laptop, 2 keyboards, 1053.8ms\n","video 1/1 (frame 77/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 2 keyboards, 1173.7ms\n","video 1/1 (frame 78/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 2 chairs, 1 laptop, 2 keyboards, 1628.1ms\n","video 1/1 (frame 79/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 2 chairs, 1 laptop, 2 keyboards, 1740.5ms\n","video 1/1 (frame 80/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 2 keyboards, 1546.0ms\n","video 1/1 (frame 81/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 2 keyboards, 1056.3ms\n","video 1/1 (frame 82/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 2 chairs, 1 laptop, 2 keyboards, 1026.4ms\n","video 1/1 (frame 83/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 2 chairs, 1 laptop, 2 keyboards, 1029.6ms\n","video 1/1 (frame 84/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 2 chairs, 1 laptop, 2 keyboards, 1049.1ms\n","video 1/1 (frame 85/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 2 keyboards, 1032.9ms\n","video 1/1 (frame 86/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 2 keyboards, 1051.2ms\n","video 1/1 (frame 87/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 laptop, 2 keyboards, 1025.9ms\n","video 1/1 (frame 88/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 laptop, 2 keyboards, 1059.2ms\n","video 1/1 (frame 89/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 laptop, 2 keyboards, 1171.5ms\n","video 1/1 (frame 90/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 laptop, 2 keyboards, 1708.6ms\n","video 1/1 (frame 91/91) /content/drive/MyDrive/Colab Notebooks/ND Face Recognition/compiled/inputs/Ashish.mp4: 640x480 1 person, 1 bottle, 1 chair, 1 laptop, 2 keyboards, 1701.9ms\n","Speed: 5.3ms preprocess, 1247.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n","Results saved to \u001b[1mruns/detect/predict30\u001b[0m\n","127.42956233024597\n","['person', 'keyboard', 'laptop', 'bottle', 'chair']\n","\n","\n","Moving to eye-blink detection.\n","\n","Total number of blinks detected: 4\n","Moving to face detection.\n","Frame no. 90: 0.970619797706604\n","24-06-07 07:14:50 - facenet512_weights.h5 will be downloaded...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading...\n","From: https://github.com/serengil/deepface_models/releases/download/v1.0/facenet512_weights.h5\n","To: /root/.deepface/weights/facenet512_weights.h5\n","100%|██████████| 95.0M/95.0M [00:00<00:00, 197MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["ACCESS DENIED: Face not verified!\n","Total time required for code running: 3.26 Minutes.\n"]}]},{"cell_type":"code","source":["for result in results[0]:\n","  for detect in result.boxes.data:\n","    print(detect)\n","    print(int(detect[5]))\n","    print(model.names[int(detect[5])])"],"metadata":{"id":"07m4oAJZMxvi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717740871134,"user_tz":-330,"elapsed":612,"user":{"displayName":"atul kashyap","userId":"02181211827561551789"}},"outputId":"3fbda239-ec7d-4408-9282-aa222280ccb6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1.6398e+00, 2.4897e+02, 1.0309e+03, 1.4383e+03, 9.6951e-01, 0.0000e+00], device='cuda:0')\n","0\n","person\n","tensor([8.9759e+02, 4.5256e+02, 1.0782e+03, 5.2730e+02, 8.8010e-01, 6.6000e+01], device='cuda:0')\n","66\n","keyboard\n","tensor([8.6826e+02, 2.5815e+02, 9.3462e+02, 4.1725e+02, 6.2714e-01, 3.9000e+01], device='cuda:0')\n","39\n","bottle\n","tensor([9.6917e+02, 2.6726e+02, 1.0794e+03, 5.0020e+02, 4.8776e-01, 6.3000e+01], device='cuda:0')\n","63\n","laptop\n","tensor([1.0120e+03, 3.6193e+02, 1.0795e+03, 4.3415e+02, 4.4585e-01, 6.6000e+01], device='cuda:0')\n","66\n","keyboard\n","tensor([4.4814e-01, 3.4749e+02, 3.4461e+02, 7.0439e+02, 4.1312e-01, 5.6000e+01], device='cuda:0')\n","56\n","chair\n"]}]},{"cell_type":"code","source":["for result in results[0]:\n","  for detect in result.boxes.data:\n","    print(detect)\n","    print(f'class id of objects: {int(detect[5])}')\n","    print(f'class id of objects: {model.names[int(detect[5])]}')\n","    print(f'probability of objects: {detect[4]}')"],"metadata":{"id":"opzIZtulbtx3","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1717741835326,"user_tz":-330,"elapsed":900,"user":{"displayName":"atul kashyap","userId":"02181211827561551789"}},"outputId":"b235beb1-0f92-40bb-ae47-7195fcbe8f4b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1.6398e+00, 2.4897e+02, 1.0309e+03, 1.4383e+03, 9.6951e-01, 0.0000e+00], device='cuda:0')\n","class id of objects: 0\n","class id of objects: person\n","probability of objects: 0.9695107340812683\n","tensor([8.9759e+02, 4.5256e+02, 1.0782e+03, 5.2730e+02, 8.8010e-01, 6.6000e+01], device='cuda:0')\n","class id of objects: 66\n","class id of objects: keyboard\n","probability of objects: 0.8800976872444153\n","tensor([8.6826e+02, 2.5815e+02, 9.3462e+02, 4.1725e+02, 6.2714e-01, 3.9000e+01], device='cuda:0')\n","class id of objects: 39\n","class id of objects: bottle\n","probability of objects: 0.627143144607544\n","tensor([9.6917e+02, 2.6726e+02, 1.0794e+03, 5.0020e+02, 4.8776e-01, 6.3000e+01], device='cuda:0')\n","class id of objects: 63\n","class id of objects: laptop\n","probability of objects: 0.4877634644508362\n","tensor([1.0120e+03, 3.6193e+02, 1.0795e+03, 4.3415e+02, 4.4585e-01, 6.6000e+01], device='cuda:0')\n","class id of objects: 66\n","class id of objects: keyboard\n","probability of objects: 0.4458509087562561\n","tensor([4.4814e-01, 3.4749e+02, 3.4461e+02, 7.0439e+02, 4.1312e-01, 5.6000e+01], device='cuda:0')\n","class id of objects: 56\n","class id of objects: chair\n","probability of objects: 0.4131247103214264\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","frames_info = pd.DataFrame(columns = ['frame_id', 'conf_score', 'object_id', 'object_name', 'x1', 'x2', 'y1', 'y2'])"],"metadata":{"id":"r_gvBngUbt0p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["frames_info"],"metadata":{"id":"aVJuS_Rrbt3B","colab":{"base_uri":"https://localhost:8080/","height":89},"executionInfo":{"status":"ok","timestamp":1717742334538,"user_tz":-330,"elapsed":7,"user":{"displayName":"atul kashyap","userId":"02181211827561551789"}},"outputId":"fabbef61-956e-4931-d868-be9f73639008"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Empty DataFrame\n","Columns: [frame_id, conf_score, object_id, object_name, x1, x2, y1, y2]\n","Index: []"],"text/html":["\n","  <div id=\"df-60ee314a-a448-41d0-b3cc-0f602d42421b\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>frame_id</th>\n","      <th>conf_score</th>\n","      <th>object_id</th>\n","      <th>object_name</th>\n","      <th>x1</th>\n","      <th>x2</th>\n","      <th>y1</th>\n","      <th>y2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-60ee314a-a448-41d0-b3cc-0f602d42421b')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-60ee314a-a448-41d0-b3cc-0f602d42421b button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-60ee314a-a448-41d0-b3cc-0f602d42421b');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","  <div id=\"id_71e507f3-ed29-4593-9faa-75dcef5d4e95\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('frames_info')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_71e507f3-ed29-4593-9faa-75dcef5d4e95 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('frames_info');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"frames_info","repr_error":"Out of range float values are not JSON compliant: nan"}},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["count = 0\n","for result in results:\n","  count+=1\n","\n","print(count)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XjCeDB4kbDmF","executionInfo":{"status":"ok","timestamp":1717742429007,"user_tz":-330,"elapsed":817,"user":{"displayName":"atul kashyap","userId":"02181211827561551789"}},"outputId":"9ec45e08-f0ae-4d61-c507-49fd07081dbc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["91\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","frames_info = pd.DataFrame(columns = ['frame_id', 'conf_score', 'object_id', 'object_name', 'x1', 'x2', 'y1', 'y2'])\n","\n","count = 1\n","\n","for result in results:\n","  for detect in result.boxes.data:\n","    detect = detect.cpu().numpy()\n","    t = pd.DataFrame({\n","        'frame_id':[count],\n","        'conf_score':[detect[4]],\n","        'object_id':[int(detect[5])],\n","        'object_name':[model.names[int(detect[5])]],\n","        'x1':[detect[0]],\n","        'x2':[detect[2]],\n","        'y1':[detect[1]],\n","        'y2':[detect[3]]\n","    })\n","\n","    frames_info = pd.concat([frames_info, t], ignore_index = True)\n","  count +=1"],"metadata":{"id":"JwJq2gfFbDon"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["frames_info.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D4wN96yDbDtW","executionInfo":{"status":"ok","timestamp":1717744565860,"user_tz":-330,"elapsed":5,"user":{"displayName":"atul kashyap","userId":"02181211827561551789"}},"outputId":"d7458b5e-b14d-4881-e7eb-9f2471148fe0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(534, 8)"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["frames_info.head(10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"id":"iEudePaubDvp","executionInfo":{"status":"ok","timestamp":1717744571241,"user_tz":-330,"elapsed":483,"user":{"displayName":"atul kashyap","userId":"02181211827561551789"}},"outputId":"e4178caa-7d50-4c77-809d-5ee70eb346d5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["  frame_id  conf_score object_id object_name           x1           x2  \\\n","0        1    0.969511         0      person     1.639778  1030.891602   \n","1        1    0.880097        66    keyboard   897.585754  1078.195801   \n","2        1    0.627143        39      bottle   868.264587   934.624756   \n","3        1    0.487764        63      laptop   969.167053  1079.366577   \n","4        1    0.445852        66    keyboard  1012.021057  1079.533447   \n","5        1    0.413124        56       chair     0.448139   344.610809   \n","6        2    0.968654         0      person     1.491943  1032.163574   \n","7        2    0.880772        66    keyboard   897.758484  1079.046631   \n","8        2    0.702380        39      bottle   868.429565   934.768005   \n","9        2    0.443866        66    keyboard  1011.044861  1079.519043   \n","\n","           y1           y2  \n","0  248.968597  1438.251465  \n","1  452.564545   527.296509  \n","2  258.145203   417.247345  \n","3  267.257629   500.196625  \n","4  361.933807   434.150513  \n","5  347.493286   704.386658  \n","6  248.618408  1438.023315  \n","7  452.493134   527.823425  \n","8  258.645966   417.718933  \n","9  360.867157   435.314148  "],"text/html":["\n","  <div id=\"df-bab7341b-de57-4442-978d-739f1a7be706\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>frame_id</th>\n","      <th>conf_score</th>\n","      <th>object_id</th>\n","      <th>object_name</th>\n","      <th>x1</th>\n","      <th>x2</th>\n","      <th>y1</th>\n","      <th>y2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0.969511</td>\n","      <td>0</td>\n","      <td>person</td>\n","      <td>1.639778</td>\n","      <td>1030.891602</td>\n","      <td>248.968597</td>\n","      <td>1438.251465</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0.880097</td>\n","      <td>66</td>\n","      <td>keyboard</td>\n","      <td>897.585754</td>\n","      <td>1078.195801</td>\n","      <td>452.564545</td>\n","      <td>527.296509</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>0.627143</td>\n","      <td>39</td>\n","      <td>bottle</td>\n","      <td>868.264587</td>\n","      <td>934.624756</td>\n","      <td>258.145203</td>\n","      <td>417.247345</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>0.487764</td>\n","      <td>63</td>\n","      <td>laptop</td>\n","      <td>969.167053</td>\n","      <td>1079.366577</td>\n","      <td>267.257629</td>\n","      <td>500.196625</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>0.445852</td>\n","      <td>66</td>\n","      <td>keyboard</td>\n","      <td>1012.021057</td>\n","      <td>1079.533447</td>\n","      <td>361.933807</td>\n","      <td>434.150513</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1</td>\n","      <td>0.413124</td>\n","      <td>56</td>\n","      <td>chair</td>\n","      <td>0.448139</td>\n","      <td>344.610809</td>\n","      <td>347.493286</td>\n","      <td>704.386658</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>2</td>\n","      <td>0.968654</td>\n","      <td>0</td>\n","      <td>person</td>\n","      <td>1.491943</td>\n","      <td>1032.163574</td>\n","      <td>248.618408</td>\n","      <td>1438.023315</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>2</td>\n","      <td>0.880772</td>\n","      <td>66</td>\n","      <td>keyboard</td>\n","      <td>897.758484</td>\n","      <td>1079.046631</td>\n","      <td>452.493134</td>\n","      <td>527.823425</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>2</td>\n","      <td>0.702380</td>\n","      <td>39</td>\n","      <td>bottle</td>\n","      <td>868.429565</td>\n","      <td>934.768005</td>\n","      <td>258.645966</td>\n","      <td>417.718933</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>2</td>\n","      <td>0.443866</td>\n","      <td>66</td>\n","      <td>keyboard</td>\n","      <td>1011.044861</td>\n","      <td>1079.519043</td>\n","      <td>360.867157</td>\n","      <td>435.314148</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bab7341b-de57-4442-978d-739f1a7be706')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-bab7341b-de57-4442-978d-739f1a7be706 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-bab7341b-de57-4442-978d-739f1a7be706');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-c207f2b5-6431-4b56-8d5b-4269bde0f18e\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c207f2b5-6431-4b56-8d5b-4269bde0f18e')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-c207f2b5-6431-4b56-8d5b-4269bde0f18e button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"frames_info","summary":"{\n  \"name\": \"frames_info\",\n  \"rows\": 534,\n  \"fields\": [\n    {\n      \"column\": \"frame_id\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 1,\n        \"max\": 91,\n        \"num_unique_values\": 91,\n        \"samples\": [\n          41,\n          23,\n          56\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"conf_score\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 534,\n        \"samples\": [\n          0.8787685036659241,\n          0.7377558350563049,\n          0.43088406324386597\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"object_id\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 0,\n        \"max\": 66,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          66,\n          56,\n          39\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"object_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"keyboard\",\n          \"chair\",\n          \"bottle\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"x1\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 471,\n        \"samples\": [\n          1.0188789367675781,\n          899.231201171875,\n          867.70458984375\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"x2\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 512,\n        \"samples\": [\n          935.6134033203125,\n          1079.2620849609375,\n          355.5959167480469\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"y1\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 534,\n        \"samples\": [\n          453.3150634765625,\n          260.3145446777344,\n          661.5094604492188\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"y2\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 532,\n        \"samples\": [\n          1438.0233154296875,\n          530.5478515625,\n          529.288818359375\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["i = 0\n","\n","for result in results:\n","  i+=1\n","\n","print(i)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LDO96tnzbDxz","executionInfo":{"status":"ok","timestamp":1717743569538,"user_tz":-330,"elapsed":608,"user":{"displayName":"atul kashyap","userId":"02181211827561551789"}},"outputId":"0d0ddb32-493d-4c30-e274-1b90fae3b769"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["91\n"]}]},{"cell_type":"code","source":["i = 0\n","\n","for result in results[0]:\n","  for detect in result.boxes.data:\n","    print(detect)\n","    i+=1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hW9cKaHEfsq2","executionInfo":{"status":"ok","timestamp":1717743936808,"user_tz":-330,"elapsed":868,"user":{"displayName":"atul kashyap","userId":"02181211827561551789"}},"outputId":"5e85f5fa-ba13-4b3d-ef70-6510550f8cf4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1.6398e+00, 2.4897e+02, 1.0309e+03, 1.4383e+03, 9.6951e-01, 0.0000e+00], device='cuda:0')\n","tensor([8.9759e+02, 4.5256e+02, 1.0782e+03, 5.2730e+02, 8.8010e-01, 6.6000e+01], device='cuda:0')\n","tensor([8.6826e+02, 2.5815e+02, 9.3462e+02, 4.1725e+02, 6.2714e-01, 3.9000e+01], device='cuda:0')\n","tensor([9.6917e+02, 2.6726e+02, 1.0794e+03, 5.0020e+02, 4.8776e-01, 6.3000e+01], device='cuda:0')\n","tensor([1.0120e+03, 3.6193e+02, 1.0795e+03, 4.3415e+02, 4.4585e-01, 6.6000e+01], device='cuda:0')\n","tensor([4.4814e-01, 3.4749e+02, 3.4461e+02, 7.0439e+02, 4.1312e-01, 5.6000e+01], device='cuda:0')\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"uvlz-BX9buak"},"execution_count":null,"outputs":[]}]}